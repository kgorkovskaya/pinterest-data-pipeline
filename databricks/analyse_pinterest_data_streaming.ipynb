{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b693868d-f603-4d00-92a8-93ca03e85b96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "AiCore Pinterest Data Pipeline Project\n",
    "Stream Pinterest data from Kinesis; clean and analyse the data with PySpark.\n",
    "This code is intended to run in a Databricks notebook.\n",
    "Author: Kristina Gorkovskaya\n",
    "Date: 2023-11-28\n",
    "'''\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark\n",
    "import tempfile\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c495d3f1-61ed-4181-96e4-000d1a040757",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Users/kristina.gorkovskaya@gmail.com/schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09bf8e23-81f3-4b91-98d5-2e3a9f7e7bfd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replace with your AWS IAM user id\n",
    "user_id ='0ec858bf1407'\n",
    "\n",
    "# Read the CSV file (credentials) to spark dataframe\n",
    "file_type = \"csv\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "aws_keys_df = spark.read.format(file_type) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(\"/FileStore/tables/authentication_credentials.csv\")\n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df \\\n",
    "  .where(col('User name')=='databricks-user') \\\n",
    "  .select('Access key ID') \\\n",
    "  .collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df \\\n",
    "  .where(col('User name')=='databricks-user') \\\n",
    "  .select('Secret access key') \\\n",
    "  .collect()[0]['Secret access key']\n",
    "\n",
    "# Encode the secret key; safe=\"\" means every char will be encoded\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "\n",
    "def read_stream(stream_name_suffix: str) -> pyspark.sql.dataframe.DataFrame:\n",
    "  '''Read Kinesis stream.'''\n",
    "\n",
    "  stream_name = f'streaming-{user_id}-{stream_name_suffix}'\n",
    "  print(f'Reading stream {stream_name}...')\n",
    "  df = spark \\\n",
    "    .readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName', stream_name) \\\n",
    "    .option('initialPosition','earliest') \\\n",
    "    .option('region','us-east-1') \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "  return df\n",
    "\n",
    "def parse_stream(stream_name_suffix: str, schemas: list) -> pyspark.sql.dataframe.DataFrame:\n",
    "  '''Read Kinesis stream and apply a schema to parse JSON data to create one column per attribute.'''\n",
    "  df = read_stream(stream_name_suffix)\n",
    "  schema = schemas[stream_name_suffix]\n",
    "  df = df.selectExpr(\"cast (data as STRING) my_json_data\").select(from_json(\"my_json_data\", schema).alias(\"tmp\")).select(\"tmp.*\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdfdce56-a597-4222-8092-d4926d9c97b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Reading stream streaming-0ec858bf1407-pin...\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Reading stream streaming-0ec858bf1407-pin...\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################################################################################\n",
    "# TASK 1: Clean the df_pin DataFrame\n",
    "#############################################################################################################\n",
    "\n",
    "df_pin = parse_stream('pin', schemas)\n",
    "\n",
    "# (1) Replace nans and empty strings with None\n",
    "df_pin = df_pin.replace(float('nan'), None).replace(\"\", None)\n",
    "\n",
    "# (2) Perform the necessary transformations on the follower_count to ensure every entry is a number.\n",
    "# Create a multiplier column\n",
    "# Assumptions: a suffix of \"k\" in follower_count means a multiplier of 1,000; a suffix of \"M\" means 10^6\n",
    "df_pin = df_pin.withColumn('follower_count_multiplier', \n",
    "                           when(df_pin.follower_count.like('%k'), 1000).\n",
    "                           when(df_pin.follower_count.like('%M'), 1000000).\n",
    "                           otherwise(1))\n",
    "\n",
    "# Parse numeric data from follower_count and apply multiplier to the parsed values\n",
    "df_pin = df_pin.withColumn('follower_count_numeric',\n",
    "                           when(df_pin.follower_count == 'User Info Error', None).\n",
    "                           otherwise(regexp_replace('follower_count', '[^0-9]+', '')).\n",
    "                           cast(IntegerType()) * df_pin.follower_count_multiplier)\n",
    "\n",
    "# Remove intermediate columns\n",
    "df_pin = df_pin.withColumn('follower_count', df_pin.follower_count_numeric)\n",
    "df_pin = df_pin.drop('follower_count_numeric', 'follower_count_multiplier')\n",
    "\n",
    "# (3) Ensure that each column containing numeric data has a numeric data type\n",
    "numeric_cols = ['downloaded', 'index']\n",
    "for c in numeric_cols:\n",
    "    df_pin = df_pin.withColumn(c, df_pin[c].cast(IntegerType()))\n",
    "\n",
    "# (4) Clean the data in the save_location column to include only the save location path\n",
    "df_pin = df_pin.withColumn('save_location', regexp_replace('save_location', 'Local save in ', ''))\n",
    "\n",
    "# (5) Rename the index column to ind, and reorder the columns.\n",
    "cols = [    \n",
    "    'ind',\n",
    "    'unique_id',\n",
    "    'title',\n",
    "    'description',\n",
    "    'follower_count',\n",
    "    'poster_name',\n",
    "    'tag_list',\n",
    "    'is_image_or_video',\n",
    "    'image_src',\n",
    "    'save_location',\n",
    "    'category'\n",
    "    ]\n",
    "df_pin = df_pin.withColumnRenamed('index', 'ind').select(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f194b78b-bc72-495a-8eed-41bf597127d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Reading stream streaming-0ec858bf1407-geo...\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Reading stream streaming-0ec858bf1407-geo...\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################################################################################\n",
    "# TASK 2: Clean the df_geo (geolocation) DataFrame\n",
    "#############################################################################################################\n",
    "\n",
    "df_geo = parse_stream('geo', schemas)\n",
    "\n",
    "# (1) Create a new column coordinates that contains an array based on the latitude and longitude columns\n",
    "df_geo = df_geo.withColumn('coordinates', array('latitude', 'longitude'))\n",
    "\n",
    "# (2) Drop the latitude and longitude columns from the DataFrame\n",
    "df_geo = df_geo.drop('latitude', 'longitude')\n",
    "\n",
    "# (3) Convert the timestamp column from a string to a timestamp data type\n",
    "df_geo = df_geo.withColumn('timestamp', to_timestamp('timestamp'))\n",
    "\n",
    "# (4) Reorder columns\n",
    "df_geo = df_geo.select('ind', 'country', 'coordinates', 'timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4851f865-4274-4b5f-89ff-b4b7648d7e21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Reading stream streaming-0ec858bf1407-user...\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Reading stream streaming-0ec858bf1407-user...\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################################################################################\n",
    "# TASK 3: Clean the df_user DataFrame\n",
    "#############################################################################################################\n",
    "\n",
    "df_user = parse_stream('user', schemas)\n",
    "\n",
    "# (1) Create a new column user_name that concatenates the information found in the first_name and last_name columns.\n",
    "# Trim and normalize whitespace.\n",
    "df_user = df_user.withColumn('user_name', regexp_replace(trim(concat_ws(' ', 'first_name', 'last_name')), '\\s+', ' '))\n",
    "\n",
    "# (2) Drop the first_name and last_name columns from the DataFrame\n",
    "df_user = df_user.drop('first_name', 'last_name')\n",
    "\n",
    "# (3) Convert the date_joined column from a string to a timestamp data type\n",
    "df_user = df_user.withColumn('date_joined', to_timestamp('date_joined'))\n",
    "\n",
    "# (4) Reorder columns.\n",
    "df_user = df_user.select('ind', 'user_name', 'age', 'date_joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a15f7ca-48b2-413f-8158-59c1da15d729",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################################################################################\n",
    "# FINAL TASK: Write transformed data to Databricks delta tables. \n",
    "#############################################################################################################\n",
    "\n",
    "# Clear delta tables\n",
    "#%fs rm -r /user/hive/warehouse/0ec858bf1407_geo_table\n",
    "#%fs rm -r /user/hive/warehouse/0ec858bf1407_pin_table\n",
    "#%fs rm -r /user/hive/warehouse/0ec858bf1407_user_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "487055f5-cc90-4a07-9c34-47842f45714c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def write_stream(df: pyspark.sql.dataframe.DataFrame, delta_table_name: str) -> None:\n",
    "  '''Write streaming data to delta table. '''\n",
    "  with tempfile.TemporaryDirectory() as d:\n",
    "    df.writeStream \\\n",
    "      .format('delta') \\\n",
    "      .outputMode('append') \\\n",
    "      .option(\"checkpointLocation\", d) \\\n",
    "      .option(\"startingOffsets\", \"earliest\") \\\n",
    "      .table(delta_table_name)  \n",
    "\n",
    "write_stream(df_geo, f'{user_id}_geo_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e253ccf-fe5f-48fc-aef2-89bcb707c298",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "write_stream(df_pin, f'{user_id}_pin_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "567b6140-756f-40d8-91a9-17efff4056f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "write_stream(df_user, f'{user_id}_user_table')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "analyse_pinterest_data_streaming",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
