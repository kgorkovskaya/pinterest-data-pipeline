{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09bf8e23-81f3-4b91-98d5-2e3a9f7e7bfd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "AiCore Pinterest Data Pipeline Project\n",
    "Stream Pinterest data from Kinesis; clean and analyse the data with PySpark.\n",
    "This code is intended to run in a Databricks notebook.\n",
    "Author: Kristina Gorkovskaya\n",
    "Date: 2023-11-10\n",
    "'''\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark\n",
    "import urllib\n",
    "\n",
    "# Replace with your AWS IAM user id\n",
    "user_id ='0ec858bf1407'\n",
    "\n",
    "# Read the CSV file (credentials) to spark dataframe\n",
    "file_type = \"csv\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "aws_keys_df = spark.read.format(file_type).option(\"header\", first_row_is_header).option(\"sep\", delimiter).load(\"/FileStore/tables/authentication_credentials.csv\")\n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']\n",
    "\n",
    "# Encode the secret key; safe=\"\" means every char will be encoded\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "\n",
    "def read_stream(stream_name_suffix) -> pyspark.sql.dataframe.DataFrame:\n",
    "  '''Read Kinesis stream.'''\n",
    "\n",
    "  stream_name = f'streaming-{user_id}-{stream_name_suffix}'\n",
    "  print(f'Reading stream {stream_name}...')\n",
    "  df = spark \\\n",
    "    .readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName', stream_name) \\\n",
    "    .option('initialPosition','earliest') \\\n",
    "    .option('region','us-east-1') \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "  return df\n",
    "\n",
    "\n",
    "def show_non_numeric_patterns(df: pyspark.sql.dataframe.DataFrame, col: str) -> pyspark.sql.dataframe.DataFrame:\n",
    "  '''Look for non-numeric patterns in a field that is expected to be numeric.'''\n",
    "  new_col = col + '_pattern'\n",
    "  df = df.withColumn(new_col, F.regexp_replace(col, '[0-9]+', '9'))\n",
    "  df.groupBy(new_col).count().show()\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca65edc2-836c-43f6-af22-80a219d0f5f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Reading stream streaming-0ec858bf1407-geo...\n",
       "Reading stream streaming-0ec858bf1407-pin...\n",
       "Reading stream streaming-0ec858bf1407-user...\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Reading stream streaming-0ec858bf1407-geo...\nReading stream streaming-0ec858bf1407-pin...\nReading stream streaming-0ec858bf1407-user...\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "df_geo = read_stream('geo')\n",
    "df_pin = read_stream('pin')\n",
    "df_user = read_stream('user')\n",
    "df_geo = df_geo.selectExpr(\"CAST(data as STRING)\")\n",
    "df_pin = df_pin.selectExpr(\"CAST(data as STRING)\")\n",
    "df_user = df_user.selectExpr(\"CAST(data as STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdfdce56-a597-4222-8092-d4926d9c97b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-2054080562114141&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     10</span> <span class=\"ansi-red-fg\"># Assumptions: a suffix of &#34;k&#34; in follower_count means a multiplier of 1,000; a suffix of &#34;M&#34; means 10^6</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     11</span> df_pin = df_pin.withColumn(&#39;follower_count_multiplier&#39;, \n",
       "<span class=\"ansi-green-fg\">---&gt; 12</span><span class=\"ansi-red-fg\">                            </span>F<span class=\"ansi-blue-fg\">.</span>when<span class=\"ansi-blue-fg\">(</span>df_pin<span class=\"ansi-blue-fg\">.</span>follower_count<span class=\"ansi-blue-fg\">.</span>like<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;%k&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1000</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     13</span>                            when<span class=\"ansi-blue-fg\">(</span>df_pin<span class=\"ansi-blue-fg\">.</span>follower_count<span class=\"ansi-blue-fg\">.</span>like<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;%M&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1000000</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     14</span>                            otherwise(1))\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">__getattr__</span><span class=\"ansi-blue-fg\">(self, name)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1798</span>         &#34;&#34;&#34;\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1799</span>         <span class=\"ansi-green-fg\">if</span> name <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1800</span><span class=\"ansi-red-fg\">             raise AttributeError(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1801</span>                 &#34;&#39;%s&#39; object has no attribute &#39;%s&#39;&#34; % (self.__class__.__name__, name))\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1802</span>         jc <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>apply<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">AttributeError</span>: &#39;DataFrame&#39; object has no attribute &#39;follower_count&#39;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2054080562114141&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     10</span> <span class=\"ansi-red-fg\"># Assumptions: a suffix of &#34;k&#34; in follower_count means a multiplier of 1,000; a suffix of &#34;M&#34; means 10^6</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span> df_pin = df_pin.withColumn(&#39;follower_count_multiplier&#39;, \n<span class=\"ansi-green-fg\">---&gt; 12</span><span class=\"ansi-red-fg\">                            </span>F<span class=\"ansi-blue-fg\">.</span>when<span class=\"ansi-blue-fg\">(</span>df_pin<span class=\"ansi-blue-fg\">.</span>follower_count<span class=\"ansi-blue-fg\">.</span>like<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;%k&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1000</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     13</span>                            when<span class=\"ansi-blue-fg\">(</span>df_pin<span class=\"ansi-blue-fg\">.</span>follower_count<span class=\"ansi-blue-fg\">.</span>like<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;%M&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1000000</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     14</span>                            otherwise(1))\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">__getattr__</span><span class=\"ansi-blue-fg\">(self, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1798</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   1799</span>         <span class=\"ansi-green-fg\">if</span> name <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1800</span><span class=\"ansi-red-fg\">             raise AttributeError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1801</span>                 &#34;&#39;%s&#39; object has no attribute &#39;%s&#39;&#34; % (self.__class__.__name__, name))\n<span class=\"ansi-green-intense-fg ansi-bold\">   1802</span>         jc <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>apply<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: &#39;DataFrame&#39; object has no attribute &#39;follower_count&#39;</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">AttributeError</span>: &#39;DataFrame&#39; object has no attribute &#39;follower_count&#39;",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################################################################################\n",
    "# TASK 1: Clean the df_pin DataFrame\n",
    "#############################################################################################################\n",
    "\n",
    "# (1) Replace nans and empty strings with None\n",
    "df_pin = df_pin.replace(float('nan'), None).replace(\"\", None)\n",
    "\n",
    "# (2) Perform the necessary transformations on the follower_count to ensure every entry is a number.\n",
    "# Create a multiplier column\n",
    "# Assumptions: a suffix of \"k\" in follower_count means a multiplier of 1,000; a suffix of \"M\" means 10^6\n",
    "df_pin = df_pin.withColumn('follower_count_multiplier', \n",
    "                           F.when(df_pin.follower_count.like('%k'), 1000).\n",
    "                           when(df_pin.follower_count.like('%M'), 1000000).\n",
    "                           otherwise(1))\n",
    "\n",
    "# Parse numeric data from follower_count and apply multiplier to the parsed values\n",
    "df_pin = df_pin.withColumn('follower_count_numeric',\n",
    "                           F.when(df_pin.follower_count == 'User Info Error', None).\n",
    "                           otherwise(F.regexp_replace('follower_count', '[^0-9]+', '')).\n",
    "                           cast(IntegerType()) * df_pin.follower_count_multiplier)\n",
    "\n",
    "# Remove intermediate columns\n",
    "df_pin = df_pin.withColumn('follower_count', df_pin.follower_count_numeric)\n",
    "df_pin = df_pin.drop('follower_count_numeric', 'follower_count_multiplier')\n",
    "\n",
    "# (3) Ensure that each column containing numeric data has a numeric data type\n",
    "numeric_cols = ['downloaded', 'index']\n",
    "for col in numeric_cols:\n",
    "    df_pin = df_pin.withColumn(col, df_pin[col].cast(IntegerType()))\n",
    "\n",
    "# (4) Clean the data in the save_location column to include only the save location path\n",
    "df_pin = df_pin.withColumn('save_location', F.regexp_replace('save_location', 'Local save in ', ''))\n",
    "\n",
    "# (5) Rename the index column to ind, and reorder the columns.\n",
    "cols = [    \n",
    "    'ind',\n",
    "    'unique_id',\n",
    "    'title',\n",
    "    'description',\n",
    "    'follower_count',\n",
    "    'poster_name',\n",
    "    'tag_list',\n",
    "    'is_image_or_video',\n",
    "    'image_src',\n",
    "    'save_location',\n",
    "    'category'\n",
    "    ]\n",
    "df_pin = df_pin.withColumnRenamed('index', 'ind').select(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f194b78b-bc72-495a-8eed-41bf597127d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################################################################################\n",
    "# TASK 2: Clean the df_geo (geolocation) DataFrame\n",
    "#############################################################################################################\n",
    "\n",
    "# (1) Create a new column coordinates that contains an array based on the latitude and longitude columns\n",
    "# Start by defining a UDF that takes multiple columns and returns an array.\n",
    "def make_list(*args):\n",
    "    return list(args)\n",
    "\n",
    "udf_make_list = F.udf(make_list, ArrayType(DoubleType()))\n",
    "\n",
    "# Then apply the UDF to latitude and longitude\n",
    "df_geo = df_geo.withColumn('coordinates', udf_make_list('latitude', 'longitude'))\n",
    "df_geo.select('latitude', 'longitude', 'coordinates').show(10, truncate=False)\n",
    "\n",
    "# (2) Drop the latitude and longitude columns from the DataFrame\n",
    "df_geo = df_geo.drop('latitude', 'longitude')\n",
    "\n",
    "# (3) Convert the timestamp column from a string to a timestamp data type\n",
    "df_geo = df_geo.withColumn('timestamp', F.to_timestamp('timestamp'))\n",
    "\n",
    "# (4) Reorder columns\n",
    "df_geo = df_geo.select('ind', 'country', 'coordinates', 'timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4851f865-4274-4b5f-89ff-b4b7648d7e21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################################################################################\n",
    "# TASK 3: Clean the df_user DataFrame\n",
    "#############################################################################################################\n",
    "# (1) Create a new column user_name that concatenates the information found in the first_name and last_name columns.\n",
    "# Trim and normalize whitespace.\n",
    "df_user = df_user.withColumn('user_name', F.regexp_replace(F.trim(F.concat_ws(' ', 'first_name', 'last_name')), '\\s+', ' '))\n",
    "\n",
    "# (2) Drop the first_name and last_name columns from the DataFrame\n",
    "df_user = df_user.drop('first_name', 'last_name')\n",
    "\n",
    "# (3) Convert the date_joined column from a string to a timestamp data type\n",
    "df_user = df_user.withColumn('date_joined', F.to_timestamp('date_joined'))\n",
    "\n",
    "# (4) Reorder columns.\n",
    "df_user = df_user.select('ind', 'user_name', 'age', 'date_joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "487055f5-cc90-4a07-9c34-47842f45714c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################################################################################\n",
    "# FINAL TASK: Write transformed data to Databricks delta tables. \n",
    "#############################################################################################################\n",
    "\n",
    "df_geo.writeStream \\\n",
    "  .format('delta') \\\n",
    "  .outputMode('append') \\\n",
    "  .table(f'{user_id}_geo_table')\n",
    "\n",
    "df_pin.writeStream \\\n",
    "  .format('delta') \\\n",
    "  .outputMode('append') \\\n",
    "  .table(f'{user_id}_pin_table')\n",
    "\n",
    "df_user.writeStream \\\n",
    "  .format('delta') \\\n",
    "  .outputMode('append') \\\n",
    "  .table(f'{user_id}_user_table')\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "analyse_pinterest_data_streaming",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
